untrusted-projects {
  databricks.spark.hipaa.enabled = false
  databricks.central.tierFlags.showSqlEndpoints = true
  com.databricks.enableNotebookWorkflows = true
}

chauffeur {
databricks.daemon.chauffeur.driver.heapSizeMB = 19372
databricks.daemon.chauffeur.executor.heapSizeMB = 20396
databricks.common.rpc.jetty.sslKeyStoreFile = "jetty-ssl-keystore.jks"
databricks.common.rpc.jetty.sslKeyCrtFile = "jetty-ssl-key-crt.pem"
databricks.daemon.chauffeur.driverUnresponsiveTimeout = 300
databricks.daemon.chauffeur.driver.spark.publicDNS = "10.127.160.73"
databricks.daemon.chauffeur.driver.spark.masterURL = "spark://10.127.160.73:7077"
databricks.common.rpc.jetty.sslDriverTrustStoreFile = "jetty-ssl-driver-truststore.jks"
databricks.common.rpc.jetty.sslTrustStoreFile = "jetty-ssl-truststore.jks"
databricks.common.rpc.jetty.sslDriverKeyStoreFile = "jetty-ssl-driver-keystore.jks"
databricks.daemon.chauffeur.driver.spark.appUiPort = 40571
databricks.daemon.chauffeur.noDriverDaemon = false
}

all-projects {
spark.databricks.clusterUsageTags.clusterNoDriverDaemon = "false"
spark.databricks.clusterUsageTags.enableElasticDisk = "false"
databricks.instance.metadata.cloudProvider = "AWS"
spark.databricks.clusterUsageTags.clusterOwnerUserId = "100003"
spark.databricks.clusterUsageTags.userProvidedRemoteVolumeSizeGb = "0"
spark.databricks.clusterUsageTags.enableJdbcAutoStart = "true"
spark.databricks.clusterUsageTags.enableJobsAutostart = "true"
spark.databricks.clusterUsageTags.clusterOwnerOrgId = "0"
spark.databricks.clusterUsageTags.clusterFirstOnDemand = "9"
spark.databricks.clusterUsageTags.driverNodeType = "i3.xlarge"
spark.databricks.clusterUsageTags.workerEnvironmentId = "default-worker-env"
spark.databricks.clusterUsageTags.sparkVersion = "5.1.x-scala2.11"
spark.databricks.clusterUsageTags.clusterEbsVolumeSize = "0"
spark.databricks.clusterUsageTags.clusterName = "brad-test"
spark.databricks.clusterUsageTags.clusterStateMessage = "Starting Spark"
spark.databricks.clusterUsageTags.driverContainerPrivateIp = "127.0.0.1"
spark.databricks.clusterUsageTags.clusterEbsVolumeCount = "0"
spark.databricks.clusterUsageTags.clusterState = "Pending"
spark.databricks.clusterUsageTags.clusterSpotBidPricePercent = "100"
spark.databricks.clusterUsageTags.clusterWorkers = "2"
spark.databricks.clusterUsageTags.clusterNumSshKeys = "0"
spark.databricks.clusterUsageTags.clusterNodeType = "i3.xlarge"
databricks.instance.metadata.instanceType = "i3.xlarge"
spark.databricks.clusterUsageTags.driverPublicDns = ""
spark.databricks.clusterUsageTags.clusterEbsVolumeType = "GENERAL_PURPOSE_SSD"
databricks.instance.metadata.hostname = ""
spark.databricks.clusterUsageTags.instanceProfileArn = "arn:aws-iso:iam::111111111111:instance-profile/DatabricksWorkerIAMRole"
spark.databricks.clusterUsageTags.clusterAllTags = "[{\"key\":\"Vendor\",\"value\":\"Databricks\"},{\"key\":\"Creator\",\"value\":\"lucasba@cia.ic.gov\"},{\"key\":\"ClusterName\",\"value\":\"brad-test\"},{\"key\":\"ClusterId\",\"value\":\"0124-151936-ails1\"},{\"key\":\"Name\",\"value\":\"dbcusttest-worker\"}]"
spark.databricks.clusterUsageTags.autoTerminationMinutes = "60"
spark.databricks.clusterUsageTags.clusterPinned = "false"
spark.databricks.clusterUsageTags.userProvidedRemoteVolumeCount = "0"
spark.databricks.clusterUsageTags.clusterLogDestination = ""
databricks.region.name = "us-iso-east-1"
databricks.instance.metadata.instanceId = "i-11111111111111111"
spark.databricks.clusterUsageTags.containerZoneId = "us-iso-east-1a"
spark.databricks.clusterUsageTags.clusterLogDeliveryEnabled = "false"
spark.databricks.clusterUsageTags.enableDfAcls = "false"
spark.databricks.workerNodeTypeId = "i3.xlarge"
spark.databricks.clusterUsageTags.driverInstanceId = "i-11111111111111111"
spark.databricks.clusterUsageTags.enableSqlAclsOnly = "false"
spark.databricks.clusterUsageTags.clusterGeneration = "18"
spark.databricks.clusterUsageTags.clusterMetastoreAccessType = "RDS_DIRECT"
spark.databricks.clusterUsageTags.clusterCreator = "Webapp"
spark.databricks.clusterUsageTags.clusterScalingType = "fixed_size"
spark.databricks.clusterUsageTags.clusterId = "0123-123456-abcd1"
spark.databricks.clusterUsageTags.clusterPythonVersion = "2"
spark.databricks.clusterSource = "UI"
spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2 = "0"
spark.databricks.clusterUsageTags.driverContainerId = "b1946ac92492d2347c6235b4d2611184"
spark.databricks.driverNodeTypeId = "i3.xlarge"
spark.databricks.clusterUsageTags.clusterResourceClass = "default"
databricks.branch.name = "abcd-pvc-branch-2.83-111-11111111"
spark.databricks.clusterUsageTags.driverInstancePrivateIp = "127.0.0.1"
spark.databricks.clusterUsageTags.clusterTargetWorkers = "2"
spark.databricks.clusterUsageTags.clusterAvailability = "ON_DEMAND"
spark.databricks.clusterUsageTags.userProvidedRemoteVolumeType = "ebs_volume_type: GENERAL_PURPOSE_SSD\n"
}

driver {
"spark.cleaner.referenceTracking.blocking" = true
"spark.shuffle.memoryFraction" = 0.2
spark.hadoop."databricks.dbfs.client.version" = "v2"
"spark.files.useFetchCache" = false
"spark.sql.hive.metastore.sharedPrefixes" = "com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,com.databricks.instrumentation,com.databricks.rpc,com.codahale,com.fasterxml.jackson,com.databricks.logging,com.databricks.UserId,com.databricks.OrgId,com.databricks.UserContext"
"spark.hadoop.hive.server2.keystore.path" = "/databricks/keys/jetty-ssl-driver-keystore.jks" "spark.sql.hive.convertMetastoreParquet" = true
databricks.daemon.driver.internalMetastore.port = 3306
spark.hadoop."spark.sql.parquet.output.committer.class" = "org.apache.spark.sql.parquet.DirectParquetOutputCommitter"
databricks.daemon.driver.internalMetastore.user = "HJNAdasOaAnwqPln"
databricks.common.rpc.jetty.sslKeyStoreFile = "jetty-ssl-keystore.jks"
"spark.speculation" = true
spark.hadoop."databricks.s3.auto-detect-endpoint" = false
spark.hadoop."fs.s3a.endpoint" = "s3.us-iso-east-1.c2s.ic.gov"
"spark.sql.parquet.cacheMetadata" = true
"spark.storage.blockManagerTimeoutIntervalMs" = 300000
databricks.daemon.driver.internalMetastore.password = "f572d396fae9206628714fb2ce00f72e94f2258f"
databricks.common.rpc.jetty.sslKeyCrtFile = "jetty-ssl-key-crt.pem"
"spark.databricks.overrideDefaultCommitProtocol" = "org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol"
"spark.databricks.io.directoryCommit.enableLogicalDelete" = false
spark.hadoop."mapred.output.committer.class" = "com.databricks.backend.daemon.data.client.DirectOutputCommitter"
"spark.streaming.driver.writeAheadLog.allowBatching" = true
databricks.s3.auto-detect-endpoint = false
"spark.worker.cleanup.enabled" = false
spark.hadoop."databricks.dbfs.client.aws.sessionTokenAllowed" = false
"spark.hadoop.fs.s3a.fast.upload.default" = true
databricks.common.rpc.jetty.sslDriverTrustStoreFile = "jetty-ssl-driver-truststore.jks"
databricks.daemon.driver.internalMetastore.poolSize = "1"
"spark.sql.hive.convertCTAS" = true
"spark.shuffle.manager" = "SORT"
"spark.driver.maxResultSize" = "4g"
databricks.workflow.sslTrustAll = true
databricks.daemon.driver.internalMetastore.database = "organization0"
databricks.common.rpc.jetty.sslTrustStoreFile = "jetty-ssl-truststore.jks"
"spark.sql.parquet.compression.codec" = "gzip"
"spark.r.numRBackendThreads" = 1
"spark.hadoop.hive.server2.keystore.password" = "dsioada7d7a"
databricks.daemon.driver.internalMetastore.host = "dbcusttest.saidoaisydoisa.rds.us-iso-east-1.c2s.ic.gov"
"spark.rdd.compress" = true
"spark.storage.memoryFraction" = 0.5
"spark.files.overwrite" = true
"spark.streaming.driver.writeAheadLog.closeFileAfterWrite" = true
"spark.akka.frameSize" = 256
databricks.common.rpc.jetty.sslDriverKeyStoreFile = "jetty-ssl-driver-keystore.jks"
spark.hadoop."databricks.s3.endpoint" = "s3.us-iso-east-1.c2s.ic.gov"
spark.hadoop."spark.sql.sources.outputCommitterClass" = "com.databricks.backend.daemon.data.client.MapReduceDirectOutputCommitter"
databricks.s3.endpoint = "s3.us-iso-east-1.c2s.ic.gov"
"spark.speculation.multiplier" = 3
"spark.speculation.quantile" = 0.9
}
